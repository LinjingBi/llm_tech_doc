{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMk5GNpvco7TgtpZqTqbG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinjingBi/llm_tech_doc/blob/main/pipeline/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/blog/how-to-generate"
      ],
      "metadata": {
        "id": "naOi70bE7Xj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE\n",
        "1. \"And that new sequence becomes the input to the model in its next step. This is an idea called â€œauto-regressionâ€\"\n",
        " -- [The Illustrated GPT-2 (Visualizing Transformer Language Models)]([https://jalammar.github.io/illustrated-gpt2/)\n"
      ],
      "metadata": {
        "id": "xr1lyGKYj98Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZSpWdl5yiD8n"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id).to(torch_device)"
      ],
      "metadata": {
        "id": "7DSXRfqPrmo2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer"
      ],
      "metadata": {
        "id": "X5ovFN0os42P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Greedy Search\n",
        "The simplest decoding method - selects the word with the hightest probability as its next word."
      ],
      "metadata": {
        "id": "-p7obj2bs7Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer('I enjoy walking with my cure dog', return_tensors='pt').to(torch_device)\n",
        "\n",
        "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
        "\n",
        "print(\"Output:\\n\" + 100*'-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd2JZBJcsu-d",
        "outputId": "5f9282d8-7fb9-49cb-9101-19502c806d20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog, but I'm not sure if I'll ever be able to walk with my dog again. I'm not sure if I'll ever be able to walk with my dog again.\n",
            "\n",
            "I'm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problems: it misses high probability words hidden behind a low probability word."
      ],
      "metadata": {
        "id": "QEe0f2bYwzgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam search\n",
        "Reduce the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choose the hypothesis that has the overall highest probability."
      ],
      "metadata": {
        "id": "KUpZksB1xAED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print('Output:\\n' + 100*'-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRr8ZyRB0fBR",
        "outputId": "01df1822-0b97-458d-ccd3-166e2fb9d489"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog, but I'm not sure if I'll ever be able to walk with him again.\n",
            "\n",
            "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is more fluent. But it is now abit like \"repeat the most likely num_beams pairs\".. So to fix the repetitions of the same word sequences, we need to introduce n-grams penalties."
      ],
      "metadata": {
        "id": "OV1VzjvwqOeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,  # no 2-gram appears twice\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print('Output:\\n' + 100*'-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QClp8RCLqLBc",
        "outputId": "533c0f6d-fa50-4bbe-c119-4bb43596089b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog, but I don't think I'll ever be able to walk with him again.\"\n",
            "\n",
            "\"I'm not going to let you down,\" she said. \"I love you, and I'm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problems with beam search:\n",
        "1. hard to find a good tradeoff between good reptition and bad reapting cycle when you need to generate long text, like a story.\n",
        "2. High quality human language does not follow a distribution of high probability next words. In other words, you can't rely on \"probability\" to generate high \"quality\"/\"interesting\" human text.\n",
        "\n",
        "So let's stop being boring and introduce some randomness ðŸ¤ª."
      ],
      "metadata": {
        "id": "1unjXkh6vefi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "8PUc1aBl7Ozl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random sampling\n",
        "\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    top_k=0,  # deactive top_k\n",
        "    do_sample=True,  # active sampling\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlB4KTERtVbu",
        "outputId": "eb0d2048-7e50-41f7-90ba-791546af8380"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog but what I love about being a dog trainer is that I get to explore with people who may treat dog handlers but I really like the custom so I feel very comfortable with that.\n",
            "\n",
            "Rosie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "not repiting, but doesn't make sense.."
      ],
      "metadata": {
        "id": "uAvUYYWLHl51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to increase the likelihood of high proabilit words and decrease\n",
        "# the likelihood of low probability words by lowering the so-called temperature of the softmax\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    top_k=0,  # deactive top_k\n",
        "    do_sample=True,  # active sampling\n",
        "    temperature=0.6  # lower temperature means more deterministic, when -> 0, it becomes greedy decoding\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbdBMcBQHjL6",
        "outputId": "32458ca5-2e5c-42c7-8433-0f9d88631f67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog, your dog. I also enjoy the cold weather. I don't know if I will ever buy another dog, but I love them both.\n",
            "\n",
            "I love that I can rely on your relationship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-K Sampling\n",
        "the K most likely next words are filtered and the probability mass is redistributed among only those K next words."
      ],
      "metadata": {
        "id": "VL3R7il4Iiih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7OSf29qIhcM",
        "outputId": "fc36b02b-177a-406d-f662-31f34b92246b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog after school! My mom would love to play with both cats and dogs and to give their cat a playtime. I've even had to leave our bed and do a game with my cat to see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One concern though with Top-K sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution\n",
        ".This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above)."
      ],
      "metadata": {
        "id": "q5Zl7MJMQaa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-p (nucleus) Sampling\n",
        "Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p.\n"
      ],
      "metadata": {
        "id": "l78WR0m3QwU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd1Gxm1UQ7ft",
        "outputId": "f99b6514-f9e2-4163-bc7d-273606d4ac22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I enjoy walking with my cure dog Abby, which I can say is a little mean but she's going for a lifetime. I know that I'm doing our utmost to be there for her and she's coming back next month,\" Hans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection."
      ],
      "metadata": {
        "id": "YbbdP3rDRZZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3kxMQEkRdPM",
        "outputId": "91126800-949c-4d30-a2c0-7672ebbb0cbb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I enjoy walking with my cure dog. If I feel the need to go on a walk myself I will be a happy man. I will walk with my cure dog, just as if it were my own. I believe that we all\n",
            "1: I enjoy walking with my cure dog, but he's very sensitive. I hope he doesn't lose his sense of humor.\"\n",
            "\n",
            "The new experiment also tested different types of dog treats given to cats and dogs. A study in the\n",
            "2: I enjoy walking with my cure dog, she is a great friend to me as it is very easy to walk with my cure dog. You have to pay attention to her and it's much more practical for them to have an easy life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Open-ended language generation is a rapidly evolving field of research and as it is often the case there is no one-size-fits-all method here, so one has to see what works best in one's specific use case. If allowed, try everything :)"
      ],
      "metadata": {
        "id": "aCwq4YEuSVpX"
      }
    }
  ]
}